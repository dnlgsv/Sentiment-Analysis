{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download NLTK data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.data.find('tokenizers/punkt'))\n",
    "print(nltk.data.find('corpora/stopwords'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ajaykarthick/imdb-movie-reviews\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First glance at the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is pretty big, so I will load only test data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(\"First few entries in the dataset:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see that review with index 4 has html tags, so let's take a closser look at it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[4]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the review has html tags, but it is not a big deal, because we can remove them easily if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label:\", df.iloc[4]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, while it is not obligatory to use \"1\" for positive and \"0\" for negative, it is a common practice. However this dataset uses \"0\" for positive and \"1\" for negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statisctics and distributions of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gladly, there are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate reviews\n",
    "duplicate_count = df.duplicated(subset='review').sum()\n",
    "print(f\"\\nNumber of duplicate reviews: {duplicate_count}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "if duplicate_count > 0:\n",
    "    df = df.drop_duplicates(subset='review').reset_index(drop=True)\n",
    "    print(\"Duplicates have been removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 16 duplicates in the dataset. While it is not a big deal, I will remove them, since they can skew analysis, models training and evaluation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Visualize class distribution\n",
    "sns.countplot(x='label', data=df)\n",
    "plt.title('Sentiment Class Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is balanced, so we don't need to worry about class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Review Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate review lengths (number of words)\n",
    "df['review_length'] = df['review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Basic statistics of review lengths\n",
    "print(\"\\nReview Length Statistics:\")\n",
    "print(df['review_length'].describe())\n",
    "\n",
    "# Visualize review length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['review_length'], bins=50, kde=True)\n",
    "plt.title('Review Length Distribution')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of review length by sentiment\n",
    "sns.boxplot(x='label', y='review_length', data=df)\n",
    "plt.title('Review Length by Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no big difference in review lengths by sentiment, so we don't need to worry about it while splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Analysis (Most Common Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Function to preprocess text data\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "df['tokens'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Common Words in Positive and Negative Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate positive and negative reviews\n",
    "positive_reviews = df[df['label'] == 'positive']\n",
    "negative_reviews = df[df['label'] == 'negative']\n",
    "\n",
    "# Get all tokens for each class\n",
    "positive_tokens = [token for tokens in positive_reviews['tokens'] for token in tokens]\n",
    "negative_tokens = [token for tokens in negative_reviews['tokens'] for token in tokens]\n",
    "\n",
    "# Get most common words\n",
    "positive_counter = Counter(positive_tokens)\n",
    "negative_counter = Counter(negative_tokens)\n",
    "\n",
    "print(\"\\nMost common words in positive reviews:\")\n",
    "print(positive_counter.most_common(20))\n",
    "\n",
    "print(\"\\nMost common words in negative reviews:\")\n",
    "print(negative_counter.most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate word cloud for positive reviews\n",
    "positive_text = ' '.join(positive_tokens)\n",
    "positive_wordcloud = WordCloud(width=800, height=400).generate(positive_text)\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Positive Reviews')\n",
    "plt.show()\n",
    "\n",
    "# Generate word cloud for negative reviews\n",
    "negative_text = ' '.join(negative_tokens)\n",
    "negative_wordcloud = WordCloud(width=800, height=400).generate(negative_text)\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Negative Reviews')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
